{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd10bec5",
   "metadata": {},
   "source": [
    "# CartPole-v0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc62cf89",
   "metadata": {},
   "source": [
    "## Load packages & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "711a939f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "state = env.reset()\n",
    "env.render()\n",
    "time.sleep(1)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1aad72c",
   "metadata": {},
   "source": [
    "## Define a testing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2e3e2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(func):\n",
    "    NUM_EPOCH = 100\n",
    "    memory_score  = np.zeros((NUM_EPOCH,))\n",
    "    done = False\n",
    "    for epoch in np.arange(NUM_EPOCH):\n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = func(state)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            score += 1\n",
    "            if epoch == NUM_EPOCH-1: # render only the last epoch\n",
    "                env.render()\n",
    "        memory_score[epoch] = score\n",
    "    print('Average Score : ' + str(np.mean(memory_score)) + ' +-' + str('{0:.2f}').format(np.std(memory_score)))\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b108c55",
   "metadata": {},
   "source": [
    "## Method 1 : (no training) Random action\n",
    "### Random action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42e9075b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Score : 9.42 +-0.71\n"
     ]
    }
   ],
   "source": [
    "def randomAction(state):\n",
    "    return np.random.randint(1)\n",
    "test(randomAction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e411e511",
   "metadata": {},
   "source": [
    "## Method 2 : (no training) Conditional action\n",
    "### Push the Cart toward the falling direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee20c33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Score : 43.06 +-8.56\n"
     ]
    }
   ],
   "source": [
    "def conditionalAction(state):\n",
    "    # state[0] : cart position\n",
    "    # state[1] : cart velocity\n",
    "    # state[2] : pole angle\n",
    "    # state[3] : pole velocity at tip\n",
    "    angle = state[2] \n",
    "    if angle > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "test(conditionalAction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59adfd3a",
   "metadata": {},
   "source": [
    "## Method 3 : Learn from lucky episodes\n",
    "### Store lucky episodes and use them to train the neural network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "258dcad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "if tf.__version__ < '2.3.0' :\n",
    "    raise Exception('tensorflow version mismatch')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b7e8d3",
   "metadata": {},
   "source": [
    "### Generate Model\n",
    "* Model Description\n",
    "|Layer|input node|output node|num param|activation function|\n",
    "| --- | --- | --- | --- | --- |\n",
    "|Input Layer | 4 | 10 | 50 | relu |\n",
    "|Hidden Layer| 10 | 10 | 110 | relu |\n",
    "|Output Layer| 10 |1 | 11 | linear |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d82e5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10,input_shape=(4,),activation='relu'),\n",
    "    tf.keras.layers.Dense(10,activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "agent.compile(optimizer='adam',\n",
    "             loss='mean_squared_error',\n",
    "             metrics='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a726578c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 10)                50        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 171\n",
      "Trainable params: 171\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "agent.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d288a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learnLucky(state):\n",
    "    if agent(np.expand_dims(state,0)) > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222b36d9",
   "metadata": {},
   "source": [
    "### Algorithm\n",
    "1. During initial learning stages (ex. first 30 stages), **Conditional action** method is used to collect lucky episodes\n",
    "2. While running **NUM_RANDOM_EPISODES** number of random episodes, store the best episode in to the **memory_best_episode**\n",
    "3. After selecting the lucky episode, train the agent(neural network) with the episode\n",
    "4. Repeat 1~3 steps\n",
    "5. After initial learning stages, use **learnLucky** function which now has sufficiently trained agent to obstain certain level of performance\n",
    "6. Now utilize **learnLucky** function to obtain lucky episode\n",
    "7. After selecting the lucky episode, train the agent(neural network) with the episode\n",
    "8. Repeat 6~7 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00e75483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Stage #000 | Learning from the best score : 67\n",
      "Learning Stage #001 | Learning from the best score : 62\n",
      "Learning Stage #002 | Learning from the best score : 61\n",
      "Learning Stage #003 | Learning from the best score : 62\n",
      "Learning Stage #004 | Learning from the best score : 68\n",
      "Learning Stage #005 | Learning from the best score : 72\n",
      "Learning Stage #006 | Learning from the best score : 62\n",
      "Learning Stage #007 | Learning from the best score : 68\n",
      "Learning Stage #008 | Learning from the best score : 62\n",
      "Learning Stage #009 | Learning from the best score : 70\n",
      "Learning Stage #010 | Learning from the best score : 68\n",
      "Learning Stage #011 | Learning from the best score : 62\n",
      "Learning Stage #012 | Learning from the best score : 67\n",
      "Learning Stage #013 | Learning from the best score : 66\n",
      "Learning Stage #014 | Learning from the best score : 63\n",
      "Learning Stage #015 | Learning from the best score : 67\n",
      "Learning Stage #016 | Learning from the best score : 61\n",
      "Learning Stage #017 | Learning from the best score : 68\n",
      "Learning Stage #018 | Learning from the best score : 67\n",
      "Learning Stage #019 | Learning from the best score : 62\n",
      "Learning Stage #020 | Learning from the best score : 67\n",
      "Learning Stage #021 | Learning from the best score : 64\n",
      "Learning Stage #022 | Learning from the best score : 64\n",
      "Learning Stage #023 | Learning from the best score : 62\n",
      "Learning Stage #024 | Learning from the best score : 68\n",
      "Learning Stage #025 | Learning from the best score : 63\n",
      "Learning Stage #026 | Learning from the best score : 64\n",
      "Learning Stage #027 | Learning from the best score : 63\n",
      "Learning Stage #028 | Learning from the best score : 68\n",
      "Learning Stage #029 | Learning from the best score : 68\n",
      "Learning Stage #030 | Learning from the best score : 61\n",
      "Learning Stage #031 | Learning from the best score : 64\n",
      "Learning Stage #032 | Learning from the best score : 64\n",
      "Learning Stage #033 | Learning from the best score : 62\n",
      "Learning Stage #034 | Learning from the best score : 68\n",
      "Learning Stage #035 | Learning from the best score : 72\n",
      "Learning Stage #036 | Learning from the best score : 64\n",
      "Learning Stage #037 | Learning from the best score : 65\n",
      "Learning Stage #038 | Learning from the best score : 64\n",
      "Learning Stage #039 | Learning from the best score : 63\n",
      "Learning Stage #040 | Learning from the best score : 61\n",
      "Learning Stage #041 | Learning from the best score : 62\n",
      "Learning Stage #042 | Learning from the best score : 62\n",
      "Learning Stage #043 | Learning from the best score : 61\n",
      "Learning Stage #044 | Learning from the best score : 61\n",
      "Learning Stage #045 | Learning from the best score : 64\n",
      "Learning Stage #046 | Learning from the best score : 72\n",
      "Learning Stage #047 | Learning from the best score : 62\n",
      "Learning Stage #048 | Learning from the best score : 62\n",
      "Learning Stage #049 | Learning from the best score : 68\n",
      "Learning Stage #050 | Learning from the best score : 77\n",
      "Learning Stage #051 | Learning from the best score : 115\n",
      "Learning Stage #052 | Learning from the best score : 177\n",
      "Learning Stage #053 | Learning from the best score : 200\n",
      "Learning Stage #054 | Learning from the best score : 200\n",
      "Learning Stage #055 | Learning from the best score : 200\n",
      "Learning Stage #056 | Learning from the best score : 200\n",
      "Learning Stage #057 | Learning from the best score : 200\n",
      "Learning Stage #058 | Learning from the best score : 200\n",
      "Learning Stage #059 | Learning from the best score : 200\n",
      "Learning Stage #060 | Learning from the best score : 200\n",
      "Learning Stage #061 | Learning from the best score : 200\n",
      "Learning Stage #062 | Learning from the best score : 200\n",
      "Learning Stage #063 | Learning from the best score : 200\n",
      "Learning Stage #064 | Learning from the best score : 200\n",
      "Learning Stage #065 | Learning from the best score : 200\n",
      "Learning Stage #066 | Learning from the best score : 200\n",
      "Learning Stage #067 | Learning from the best score : 200\n",
      "Learning Stage #068 | Learning from the best score : 200\n",
      "Learning Stage #069 | Learning from the best score : 200\n",
      "Learning Stage #070 | Learning from the best score : 200\n",
      "Learning Stage #071 | Learning from the best score : 200\n",
      "Learning Stage #072 | Learning from the best score : 200\n",
      "Learning Stage #073 | Learning from the best score : 200\n",
      "Learning Stage #074 | Learning from the best score : 200\n",
      "Learning Stage #075 | Learning from the best score : 200\n",
      "Learning Stage #076 | Learning from the best score : 200\n",
      "Learning Stage #077 | Learning from the best score : 200\n",
      "Learning Stage #078 | Learning from the best score : 200\n",
      "Learning Stage #079 | Learning from the best score : 200\n",
      "Learning Stage #080 | Learning from the best score : 200\n",
      "Learning Stage #081 | Learning from the best score : 200\n",
      "Learning Stage #082 | Learning from the best score : 200\n",
      "Learning Stage #083 | Learning from the best score : 200\n",
      "Learning Stage #084 | Learning from the best score : 200\n",
      "Learning Stage #085 | Learning from the best score : 200\n",
      "Learning Stage #086 | Learning from the best score : 200\n",
      "Learning Stage #087 | Learning from the best score : 200\n",
      "Learning Stage #088 | Learning from the best score : 200\n",
      "Learning Stage #089 | Learning from the best score : 200\n",
      "Learning Stage #090 | Learning from the best score : 200\n",
      "Learning Stage #091 | Learning from the best score : 200\n",
      "Learning Stage #092 | Learning from the best score : 200\n",
      "Learning Stage #093 | Learning from the best score : 200\n",
      "Learning Stage #094 | Learning from the best score : 200\n",
      "Learning Stage #095 | Learning from the best score : 200\n",
      "Learning Stage #096 | Learning from the best score : 200\n",
      "Learning Stage #097 | Learning from the best score : 200\n",
      "Learning Stage #098 | Learning from the best score : 200\n",
      "Learning Stage #099 | Learning from the best score : 200\n",
      "Average Score : 200.0 +-0.00\n"
     ]
    }
   ],
   "source": [
    "NUM_LEARNING = 100\n",
    "NUM_RANDOM_EPISODES = 100\n",
    "MAXIMUM_REWARD = 200\n",
    "\n",
    "for learn in np.arange(NUM_LEARNING):\n",
    "    memory_best_episode = np.zeros((MAXIMUM_REWARD,5))\n",
    "    memory_best_score = 0\n",
    "    for ep in np.arange(NUM_RANDOM_EPISODES): # Run random episodes to pick a lucky episode\n",
    "        score = 0\n",
    "        temporal_memory_episode = np.zeros((MAXIMUM_REWARD,5))\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            if learn < 50:\n",
    "                action = conditionalAction(state)\n",
    "            else:\n",
    "                action = learnLucky(state)\n",
    "            temporal_memory_episode[score,:] = np.hstack((state,action)) # memorize\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            score += 1\n",
    "        if memory_best_score < score:\n",
    "            memory_best_episode = temporal_memory_episode.copy()\n",
    "            memory_best_score = score\n",
    "            ep = 2\n",
    "            if score == MAXIMUM_REWARD:\n",
    "                ep = 4\n",
    "                break\n",
    "    # after memorizing the best episode, start learn\n",
    "    print(\"Learning Stage #\" + str('{0:03d}').format(learn) + \" | Learning from the best score : \" + str(memory_best_score))\n",
    "    agent.fit(memory_best_episode[0:memory_best_score,0:4],memory_best_episode[0:memory_best_score,4]*2-1,epochs=ep,verbose=0)\n",
    "    \n",
    "test(learnLucky)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123120ba",
   "metadata": {},
   "source": [
    "## Method 4 : Q-Learning, Actor-Critic method, Deep Q-Learning, etc...."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
